{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebee152-4e5b-4904-8684-4b661b88d8e7",
   "metadata": {},
   "source": [
    "The goal of this notebook is to implement Long Short-Term memory. See:\n",
    "\n",
    "S. Hochreiter and J. Schmidhuber, \"Long Short-Term Memory,\" in Neural Computation, vol. 9, no. 8, pp. 1735-1780, 15 Nov. 1997, doi: 10.1162/neco.1997.9.8.1735.\n",
    "\n",
    "I implemented this in PyTorch borrowing parameters from: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cccb50cd-9fbd-49b1-868d-60fe43d262e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import torch\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from letsbuildmodels.devices import get_device\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import Embedding\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LSTM\n",
    "from torch.nn import Module\n",
    "from torch.nn import Sigmoid\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690d80d5-ee72-4f58-83c0-fb71c6c4c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8c131b-1482-4787-81a8-166dbb8af121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5a51eb-f2ca-4e6a-a3f1-58fa39a14b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "device = \"cpu\"\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.int32, device=device),\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.int32, device=device),\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852b6572-8457-40a3-871c-cf1f85f42a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(\n",
      "  (embedding): Embedding(5000, 32)\n",
      "  (lstm): LSTM(32, 100, batch_first=True)\n",
      "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "class Sentiment(Module):\n",
    "    def __init__(self):\n",
    "        super(Sentiment, self).__init__()\n",
    "        self.embedding = Embedding(top_words, embedding_vecor_length)\n",
    "        self.lstm = LSTM(embedding_vecor_length, 100, batch_first=True)\n",
    "        self.linear = Linear(100, 1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = hidden[0, :, :]\n",
    "        outputs = self.linear(hidden)\n",
    "        logits = self.sigmoid(outputs)\n",
    "        return torch.flatten(logits)\n",
    "        \n",
    "model = Sentiment()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4be127-a6fc-49e0-bf96-b0712fce9b07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.009957145146131515, Accuracy: 62.44%\n",
      "Epoch 2, Loss: 0.008446183835268021, Accuracy: 72.88%\n",
      "Epoch 3, Loss: 0.007413450375795365, Accuracy: 77.23%\n",
      "Epoch 4, Loss: 0.006604318021535873, Accuracy: 80.80%\n",
      "Epoch 5, Loss: 0.0063955654078722, Accuracy: 81.90%\n",
      "Epoch 6, Loss: 0.006536234651207924, Accuracy: 81.30%\n",
      "Epoch 7, Loss: 0.005538897587060929, Accuracy: 84.76%\n",
      "Epoch 8, Loss: 0.006512709921002388, Accuracy: 81.09%\n",
      "Epoch 9, Loss: 0.00595564430475235, Accuracy: 83.32%\n",
      "Epoch 10, Loss: 0.004802551005482674, Accuracy: 87.43%\n"
     ]
    }
   ],
   "source": [
    "def to_prediction(outputs):\n",
    "    return (outputs > 0.5).float()\n",
    "    \n",
    "def fit():\n",
    "    criterion = BCELoss()\n",
    "    optimizer = Adam(model.parameters())\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        epoch_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        count = 0\n",
    "        batch = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            predictions = to_prediction(outputs)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            count += inputs.size()[0]\n",
    "            batch += 1\n",
    "            print(f\"Batch {batch}, size = {inputs.size()[0]}\", end = \"\\r\")\n",
    "        avg_loss = epoch_loss / count\n",
    "        avg_accuracy = correct_predictions / count\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss}, Accuracy: {avg_accuracy:.2%}\")\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8fd3d7d-88c5-4c66-825f-a1075bed831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.052% 40\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch = 0\n",
    "        for inputs, labels in testloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            batch += 1\n",
    "            print(f\"Batch {batch}, size = {inputs.size()[0]}\", end = \"\\r\")\n",
    "        print(f\"Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
