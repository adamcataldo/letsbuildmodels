{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50d5404-3bd4-40b4-8661-e5e75a0875ed",
   "metadata": {},
   "source": [
    "This notebook implements an RNN encoder / decoder for machine translation, inspired by:\n",
    "\n",
    "[Learning phrase representations using RNN encoder-decoder for statistical machine translation](https://arxiv.org/abs/1406.1078)\n",
    "K Cho, B Van Merriënboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio\n",
    "arXiv preprint arXiv:1406.1078, 2014•arxiv.org\n",
    "\n",
    "and \n",
    "\n",
    "[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "I Sutskever - arXiv preprint arXiv:1409.3215, 2014 - jeremy-su1.github.io\n",
    "\n",
    "The implementation is influenced by:\n",
    "\n",
    "[https://colab.research.google.com/drive/1GBC7eLlEM-HqKLUuMcFIQdVuYXzLoS_P?usp=sharing](https://colab.research.google.com/drive/1GBC7eLlEM-HqKLUuMcFIQdVuYXzLoS_P?usp=sharing)\n",
    "\n",
    "Importantly, it provides the English to Italian data set I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca68a146-e884-4f02-bffd-a8fdba9166a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from letsbuildmodels.devices import get_device\n",
    "from nltk.lm.vocabulary import Vocabulary\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import one_hot\n",
    "from torcheval.metrics.functional import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c86ecac-3456-481f-a827-ef9b732913b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/jamescataldo/Code/letsbuildmodels/notebooks/encdec/data/eng_ita_v2.txt. No download needed.\n",
      "File contains 120746 translations\n",
      "English vocabulary size: 4887\n",
      "Italian vocabulary size: 9275\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "\n",
    "# Download the data\n",
    "local_path = os.path.join(os.getcwd(), \"data\", \"eng_ita_v2.txt\")\n",
    "\n",
    "def download_file_if_not_exists():\n",
    "    url = \"https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/datasets/eng_ita_v2.txt\"\n",
    "    \n",
    "    directory = os.path.dirname(local_path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"Downloading file from {url} to {local_path}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        with open(local_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"File already exists at {local_path}. No download needed.\")\n",
    "\n",
    "download_file_if_not_exists()\n",
    "\n",
    "\n",
    "# Read the data\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().strip().split('\\n')\n",
    "    pairs = [tuple([s for s in line.split(' -> ')]) for line in lines]\n",
    "    return pairs\n",
    "\n",
    "pairs = read_data(local_path)\n",
    "print(f\"File contains {len(pairs)} translations\")\n",
    "\n",
    "# Build vocabularies\n",
    "def build_vocab(pairs):\n",
    "    eng_tokens = list(itertools.chain.from_iterable([word_tokenize(eng) for (eng, _) in pairs]))\n",
    "    ita_tokens = list(itertools.chain.from_iterable([word_tokenize(ita) for (_, ita) in pairs]))\n",
    "    eng_length = max([len(x) for x in eng_tokens]) + 1 # +1 accounts for <EOS>\n",
    "    ita_length = max([len(x) for x in eng_tokens]) + 1\n",
    "    eng_vocab = Vocabulary(eng_tokens, unk_cutoff=2)\n",
    "    ita_vocab = Vocabulary(ita_tokens, unk_cutoff=2)\n",
    "    return eng_vocab, ita_vocab, eng_length, ita_length\n",
    "    \n",
    "eng_vocab, ita_vocab, eng_length, ita_length = build_vocab(pairs)\n",
    "\n",
    "print('English vocabulary size:', len(eng_vocab))\n",
    "print('Italian vocabulary size:', len(ita_vocab))\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Creating integer <-> word mapping\n",
    "class WordMapping:\n",
    "    def __init__(self, vocab):\n",
    "        self.word_to_int = {}\n",
    "        self.int_to_word = {}\n",
    "        word_counts = [(word, vocab[word]) for word in vocab]\n",
    "        sorted_word_counts = sorted(word_counts, key=lambda t: t[1], reverse=True)\n",
    "        sorted_word_counts = sorted_word_counts + [(PAD_TOKEN, 1), (EOS_TOKEN, 1), (SOS_TOKEN, 1)]\n",
    "        for i, (word, _) in enumerate(sorted_word_counts):\n",
    "            self.word_to_int[word] = i\n",
    "            self.int_to_word[i] = word\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if type(key) == str:\n",
    "            if key in self.word_to_int:\n",
    "                return self.word_to_int[key]\n",
    "            elif key.lower() in self.word_to_int:\n",
    "                return self.word_to_int[key.lower()]\n",
    "            else:\n",
    "                return self.word_to_int[UNK_TOKEN]                \n",
    "        elif type(key) == int:\n",
    "            return self.int_to_word[key]\n",
    "        else:\n",
    "            raise KeyError(f\"Invalid key type: {type(key)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_int)\n",
    "\n",
    "eng_mapping = WordMapping(eng_vocab)\n",
    "ita_mapping = WordMapping(ita_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb4b47f-9dae-4d7c-80c6-3e8f38de2e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training translations: 120746\n",
      "Number of batches: 943\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets and loaders\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng, ita = self.pairs[idx]\n",
    "        eng_tensor = torch.tensor([eng_mapping[word] for word in word_tokenize(eng)]\n",
    "                                  + [eng_mapping[EOS_TOKEN]], dtype=torch.long)\n",
    "        ita_tensor = torch.tensor([ita_mapping[word] for word in word_tokenize(ita)]\n",
    "                                  + [ita_mapping[EOS_TOKEN]], dtype=torch.long)\n",
    "        return eng_tensor, ita_tensor\n",
    "\n",
    "# Custom collate function to handle padding\n",
    "def collate_fn(batch):\n",
    "    eng_batch, ita_batch = zip(*batch)\n",
    "    eng_batch_padded = pad_sequence(eng_batch, batch_first=True, padding_value=eng_mapping[PAD_TOKEN])\n",
    "    ita_batch_padded = pad_sequence(ita_batch, batch_first=True, padding_value=ita_mapping[PAD_TOKEN])\n",
    "    return eng_batch_padded, ita_batch_padded\n",
    "\n",
    "# Create the DataLoader\n",
    "translation_dataset = TranslationDataset()\n",
    "translations = len(translation_dataset)\n",
    "indices = list(range(translations))\n",
    "train_indices = indices[::2]\n",
    "test_indices = indices[1::2]\n",
    "train_dataset = Subset(translation_dataset, train_indices)\n",
    "test_dataset = Subset(translation_dataset, test_indices)\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "batches = len(train_dataloader)\n",
    "print(f\"Training translations: {translations}\")\n",
    "print(f\"Number of batches: {batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92a11af4-8aff-4129-b48b-38e8420420d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(4887, 256)\n",
      "    (gru): GRU(256, 512, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(9275, 256)\n",
      "    (gru): GRU(256, 512, batch_first=True)\n",
      "    (linear): Linear(in_features=512, out_features=9275, bias=True)\n",
      "    (softmax): Softmax(dim=2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the models\n",
    "eng_vocab_size = len(eng_vocab)\n",
    "ita_vocab_size = len(ita_vocab)\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "max_output_length = 32\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(eng_vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        reversed_x = torch.flip(x, (1,))\n",
    "        embedded = self.embedding(reversed_x)\n",
    "        _, hidden = self.gru(embedded)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(ita_vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size,\n",
    "                           hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, ita_vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        decoder_gru, _ = self.gru(embedded, hidden)\n",
    "        out = self.linear(decoder_gru)\n",
    "        return self.softmax(out), hidden\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, enc_x, dec_x):\n",
    "        hidden = self.encoder(enc_x)\n",
    "        out, _ = self.decoder(dec_x, hidden)\n",
    "        return out\n",
    "\n",
    "    def translate(self, enc_x, prediction_length):\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = enc_x.size()[0]\n",
    "        translations = torch.full(\n",
    "            (batch_size, prediction_length),\n",
    "            ita_mapping[PAD_TOKEN],\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        )\n",
    "        dec_x = torch.full(\n",
    "            (batch_size, 1),\n",
    "            ita_mapping[SOS_TOKEN],\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        )\n",
    "        hidden = self.encoder(enc_x)\n",
    "        for i in range(prediction_length):\n",
    "            out, hidden = self.decoder(dec_x, hidden)\n",
    "            dec_x = torch.argmax(out, dim=2)\n",
    "            translations[:, i] = torch.flatten(dec_x)\n",
    "        return translations\n",
    "                \n",
    "\n",
    "model = Translator()\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e0da222-44c5-44b5-9280-81734facdd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6076                             \n",
      "Epoch 1, Loss: 0.6013                             \n",
      "Epoch 2, Loss: 0.5975                             \n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=ita_mapping[PAD_TOKEN])\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    num_epochs = 3\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        count = 0\n",
    "        for i, (encoder_input, target) in enumerate(train_dataloader):\n",
    "            encoder_input, target = encoder_input.to(device), target.to(device)\n",
    "            decoder_input = torch.empty_like(target)\n",
    "            decoder_input[:, 0] = ita_mapping[SOS_TOKEN]\n",
    "            decoder_input[:, 1:] = target[:, :-1]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(encoder_input, decoder_input)\n",
    "\n",
    "            loss = loss_fn(output.permute(0, 2, 1), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() / target.size()[1]\n",
    "            count += 1\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {i}/{batches}, Loss: {epoch_loss / count:.4f}\", end=\"\\r\")\n",
    "            \n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss / count:.4f}                             \")\n",
    "                \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a960ba0-b4ce-4a6a-8b0c-a377b28d4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.0407                             \n"
     ]
    }
   ],
   "source": [
    "specials = {\n",
    "    PAD_TOKEN,\n",
    "    EOS_TOKEN,\n",
    "    SOS_TOKEN,\n",
    "    UNK_TOKEN,\n",
    "}\n",
    "\n",
    "def to_ita_sentence(tensor):\n",
    "    ita = [ita_mapping[x.item()] for x in tensor]\n",
    "    strs = [y for y in ita if y not in specials]\n",
    "    return \" \".join(strs)\n",
    "\n",
    "def to_eng_sentence(tensor):\n",
    "    eng = [eng_mapping[x.item()] for x in tensor]\n",
    "    strs = [y for y in eng if y not in specials]\n",
    "    return \" \".join(strs).replace(\" '\", \"'\")\n",
    "\n",
    "def test(print_translations=False):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (encoder_input, target) in enumerate(test_dataloader):\n",
    "            encoder_input, target = encoder_input.to(device), target.to(device)\n",
    "            \n",
    "            output = model.translate(encoder_input, target.size()[1])\n",
    "    \n",
    "            for batch in range(batch_size):\n",
    "                input_tokens = encoder_input[batch]\n",
    "                input_str = to_eng_sentence(input_tokens)\n",
    "                output_str = to_ita_sentence(output[batch])            \n",
    "                target_tokens = target[batch]\n",
    "                target_str = to_ita_sentence(target_tokens) \n",
    "                if print_translations:\n",
    "                    print(f\"English: {input_str}\")\n",
    "                    print(f\"Desired Italian: {target_str}\")\n",
    "                    print(f\"Generated Italian: {output_str}\")\n",
    "                    print()\n",
    "    \n",
    "                loss = bleu_score(output_str, [target_str], n_gram=min(2, len(output_str)))\n",
    "        \n",
    "                epoch_loss += loss.item()\n",
    "                count += 1\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Batch {i}/{batches}, BLEU: {epoch_loss / count:.4f}\", end=\"\\r\")\n",
    "        print(f\"BLEU: {epoch_loss / count:.4f}                             \")\n",
    "                \n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
