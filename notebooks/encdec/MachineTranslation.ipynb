{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50d5404-3bd4-40b4-8661-e5e75a0875ed",
   "metadata": {},
   "source": [
    "This notebook implements an RNN encoder / decoder for machine translation, inspired by:\n",
    "\n",
    "[Learning phrase representations using RNN encoder-decoder for statistical machine translation](https://arxiv.org/abs/1406.1078)\n",
    "K Cho, B Van Merriënboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio\n",
    "arXiv preprint arXiv:1406.1078, 2014•arxiv.org\n",
    "\n",
    "and \n",
    "\n",
    "[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "I Sutskever - arXiv preprint arXiv:1409.3215, 2014 - jeremy-su1.github.io\n",
    "\n",
    "The implementation is heavily influenced by:\n",
    "\n",
    "https://colab.research.google.com/drive/1GBC7eLlEM-HqKLUuMcFIQdVuYXzLoS_P?usp=sharing\n",
    "\n",
    "Importantly, it provides the English to Italian data set I use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ca68a146-e884-4f02-bffd-a8fdba9166a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from letsbuildmodels.devices import get_device\n",
    "from nltk.lm.vocabulary import Vocabulary\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import one_hot\n",
    "from torcheval.metrics.functional import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8c86ecac-3456-481f-a827-ef9b732913b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/jamescataldo/Code/letsbuildmodels/notebooks/encdec/data/eng_ita_v2.txt. No download needed.\n",
      "File contains 120746 translations\n",
      "English vocabulary size: 4894\n",
      "Italian vocabulary size: 13675\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "\n",
    "# Download the data\n",
    "local_path = os.path.join(os.getcwd(), \"data\", \"eng_ita_v2.txt\")\n",
    "\n",
    "def download_file_if_not_exists():\n",
    "    url = \"https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/datasets/eng_ita_v2.txt\"\n",
    "    \n",
    "    directory = os.path.dirname(local_path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"Downloading file from {url} to {local_path}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        with open(local_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"File already exists at {local_path}. No download needed.\")\n",
    "\n",
    "download_file_if_not_exists()\n",
    "\n",
    "\n",
    "# Read the data\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().strip().split('\\n')\n",
    "    pairs = [tuple([s for s in line.split(' -> ')]) for line in lines]\n",
    "    return pairs\n",
    "\n",
    "pairs = read_data(local_path)\n",
    "print(f\"File contains {len(pairs)} translations\")\n",
    "\n",
    "# Build vocabularies\n",
    "def build_vocab(pairs):\n",
    "    eng_tokens = list(itertools.chain.from_iterable([word_tokenize(eng) for (eng, _) in pairs]))\n",
    "    ita_tokens = list(itertools.chain.from_iterable([word_tokenize(ita) for (_, ita) in pairs]))\n",
    "    eng_vocab = Vocabulary(eng_tokens)\n",
    "    ita_vocab = Vocabulary(ita_tokens)\n",
    "    return eng_vocab, ita_vocab\n",
    "\n",
    "eng_vocab, ita_vocab = build_vocab(pairs)\n",
    "\n",
    "print('English vocabulary size:', len(eng_vocab))\n",
    "print('Italian vocabulary size:', len(ita_vocab))\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Creating integer <-> word mapping\n",
    "class WordMapping:\n",
    "    def __init__(self, vocab):\n",
    "        self.word_to_int = {}\n",
    "        self.int_to_word = {}\n",
    "        word_counts = [(word, vocab[word]) for word in vocab]\n",
    "        sorted_word_counts = sorted(word_counts, key=lambda t: t[1], reverse=True)\n",
    "        sorted_word_counts = sorted_word_counts + [(PAD_TOKEN, 1), (EOS_TOKEN, 1), (SOS_TOKEN, 1)]\n",
    "        for i, (word, _) in enumerate(sorted_word_counts):\n",
    "            self.word_to_int[word] = i\n",
    "            self.int_to_word[i] = word\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if type(key) == str:\n",
    "            if key in self.word_to_int:\n",
    "                return self.word_to_int[key]\n",
    "            elif key.lower() in self.word_to_int:\n",
    "                return self.word_to_int[key.lower()]\n",
    "            else:\n",
    "                return self.word_to_int[UNK_TOKEN]                \n",
    "        elif type(key) == int:\n",
    "            return self.int_to_word[key]\n",
    "        else:\n",
    "            raise KeyError(f\"Invalid key type: {type(key)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_int)\n",
    "\n",
    "eng_mapping = WordMapping(eng_vocab)\n",
    "ita_mapping = WordMapping(ita_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "efb4b47f-9dae-4d7c-80c6-3e8f38de2e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training translations: 120746\n",
      "Number of batches: 943\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets and loaders\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng, ita = self.pairs[idx]\n",
    "        eng_tensor = torch.tensor([eng_mapping[word] for word in word_tokenize(eng)]\n",
    "                                  + [eng_mapping[EOS_TOKEN]], dtype=torch.long)\n",
    "        ita_tensor = torch.tensor([ita_mapping[word] for word in word_tokenize(ita)]\n",
    "                                  + [ita_mapping[EOS_TOKEN]], dtype=torch.long)\n",
    "        return eng_tensor, ita_tensor\n",
    "\n",
    "# Custom collate function to handle padding\n",
    "def collate_fn(batch):\n",
    "    eng_batch, ita_batch = zip(*batch)\n",
    "    eng_batch_padded = pad_sequence(eng_batch, batch_first=True, padding_value=eng_mapping[PAD_TOKEN])\n",
    "    ita_batch_padded = pad_sequence(ita_batch, batch_first=True, padding_value=ita_mapping[PAD_TOKEN])\n",
    "    return eng_batch_padded, ita_batch_padded\n",
    "\n",
    "# Create the DataLoader\n",
    "translation_dataset = TranslationDataset()\n",
    "translations = len(translation_dataset)\n",
    "train_dataset, test_dataset = random_split(translation_dataset, [translations // 2, translations // 2])\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "batches = len(train_dataloader)\n",
    "print(f\"Training translations: {translations}\")\n",
    "print(f\"Number of batches: {batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "92a11af4-8aff-4129-b48b-38e8420420d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(4894, 256)\n",
      "    (gru): GRU(256, 512, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(13675, 256)\n",
      "    (gru): GRU(256, 512, batch_first=True)\n",
      "    (linear): Linear(in_features=512, out_features=13675, bias=True)\n",
      "    (softmax): Softmax(dim=2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the models\n",
    "eng_vocab_size = len(eng_vocab)\n",
    "ita_vocab_size = len(ita_vocab)\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(eng_vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, hidden = self.gru(embedded)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(ita_vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size,\n",
    "                           hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, ita_vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        decoder_gru, _ = self.gru(embedded, hidden)\n",
    "        out = self.linear(decoder_gru)\n",
    "        return self.softmax(out)\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, enc_x, dec_x):\n",
    "        hidden = self.encoder(enc_x)\n",
    "        out = self.decoder(dec_x, hidden)\n",
    "        return out\n",
    "\n",
    "model = Translator()\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5e0da222-44c5-44b5-9280-81734facdd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.63783, Loss: 0.6378\n",
      "Epoch 1, Loss: 0.63123, Loss: 0.6311\n",
      "Epoch 2, Loss: 0.62843, Loss: 0.6283\n",
      "Epoch 3, Loss: 0.62963, Loss: 0.6293\n",
      "Epoch 4, Loss: 0.62903, Loss: 0.6290\n",
      "Epoch 5, Loss: 0.62443, Loss: 0.6245\n",
      "Epoch 6, Loss: 0.62253, Loss: 0.6229\n",
      "Epoch 7, Loss: 0.62153, Loss: 0.6216\n",
      "Epoch 8, Loss: 0.62123, Loss: 0.6212\n",
      "Epoch 9, Loss: 0.62273, Loss: 0.6225\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    loss_fn = nn.BLEUScore(ignore_index=ita_mapping[PAD_TOKEN])\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    num_epochs = 10\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        count = 0\n",
    "        for i, (encoder_input, target) in enumerate(train_dataloader):\n",
    "            encoder_input, target = encoder_input.to(device), target.to(device)\n",
    "            decoder_input = torch.empty_like(target)\n",
    "            decoder_input[:, 0] = ita_mapping[SOS_TOKEN]\n",
    "            decoder_input[:, 1:] = target[:, :-1]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(encoder_input, decoder_input)\n",
    "\n",
    "            loss = loss_fn(output.permute(0, 2, 1), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() / target.size()[1]\n",
    "            count += 1\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {i}/{batches}, Loss: {epoch_loss / count:.4f}\", end=\"\\r\")\n",
    "            \n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss / count:.4f}                             \")\n",
    "                \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1a960ba0-b4ce-4a6a-8b0c-a377b28d4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 190/943, BLEU: 0.0626"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[271], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, BLEU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mcount\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mcount\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m                             \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[271], line 36\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(print_translations)\u001b[0m\n\u001b[1;32m     34\u001b[0m input_str \u001b[38;5;241m=\u001b[39m to_eng_sentence(input_tokens)\n\u001b[1;32m     35\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output[batch], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m output_str \u001b[38;5;241m=\u001b[39m \u001b[43mto_ita_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_tokens\u001b[49m\u001b[43m)\u001b[49m            \n\u001b[1;32m     37\u001b[0m target_tokens \u001b[38;5;241m=\u001b[39m target[batch]\n\u001b[1;32m     38\u001b[0m target_str \u001b[38;5;241m=\u001b[39m to_ita_sentence(target_tokens) \n",
      "Cell \u001b[0;32mIn[271], line 9\u001b[0m, in \u001b[0;36mto_ita_sentence\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_ita_sentence\u001b[39m(tensor):\n\u001b[0;32m----> 9\u001b[0m     ita \u001b[38;5;241m=\u001b[39m [ita_mapping[\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensor]\n\u001b[1;32m     10\u001b[0m     strs \u001b[38;5;241m=\u001b[39m [y \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ita \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m specials]\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(strs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "specials = {\n",
    "    PAD_TOKEN,\n",
    "    EOS_TOKEN,\n",
    "    SOS_TOKEN,\n",
    "    UNK_TOKEN,\n",
    "}\n",
    "\n",
    "def to_ita_sentence(tensor):\n",
    "    ita = [ita_mapping[x.item()] for x in tensor]\n",
    "    strs = [y for y in ita if y not in specials]\n",
    "    return \" \".join(strs)\n",
    "\n",
    "def to_eng_sentence(tensor):\n",
    "    eng = [eng_mapping[x.item()] for x in tensor]\n",
    "    strs = [y for y in eng if y not in specials]\n",
    "    return \" \".join(strs).replace(\" '\", \"'\")\n",
    "\n",
    "def test(print_translations=False):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (encoder_input, target) in enumerate(test_dataloader):\n",
    "            encoder_input, target = encoder_input.to(device), target.to(device)\n",
    "            decoder_input = torch.empty_like(target)\n",
    "            decoder_input[:, 0] = ita_mapping[SOS_TOKEN]\n",
    "            decoder_input[:, 1:] = target[:, :-1]\n",
    "            \n",
    "            output = model(encoder_input, decoder_input)\n",
    "    \n",
    "            for batch in range(batch_size):\n",
    "                input_tokens = encoder_input[batch]\n",
    "                input_str = to_eng_sentence(input_tokens)\n",
    "                output_tokens = torch.argmax(output[batch], dim=1)\n",
    "                output_str = to_ita_sentence(output_tokens)            \n",
    "                target_tokens = target[batch]\n",
    "                target_str = to_ita_sentence(target_tokens) \n",
    "                if print_translations:\n",
    "                    print(f\"English: {input_str}\")\n",
    "                    print(f\"Desired Italian: {target_str}\")\n",
    "                    print(f\"Generated Italian: {output_str}\")\n",
    "                    print()\n",
    "    \n",
    "                loss = bleu_score(output_str, [target_str], n_gram=min(2, len(output_str)))\n",
    "        \n",
    "                epoch_loss += loss.item()\n",
    "                count += 1\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Batch {i}/{batches}, BLEU: {epoch_loss / count:.4f}\", end=\"\\r\")\n",
    "        print(f\"BLEU: {epoch_loss / count:.4f}                             \")\n",
    "                \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec91884-b7b4-4ce4-8066-92916ff00fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
