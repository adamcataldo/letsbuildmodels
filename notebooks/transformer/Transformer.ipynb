{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fd7dc2-8599-4a0e-a13c-3b38060ed17b",
   "metadata": {},
   "source": [
    "This notebook implements a transformer for machine translation, inspired by:\n",
    "\n",
    "[Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "A Vaswani - Advances in Neural Information Processing Systems, 2017\n",
    "\n",
    "and \n",
    "\n",
    "[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "I Sutskever - arXiv preprint arXiv:1409.3215, 2014 - jeremy-su1.github.io\n",
    "\n",
    "The data was grabbed from:\n",
    "\n",
    "[https://colab.research.google.com/drive/1GBC7eLlEM-HqKLUuMcFIQdVuYXzLoS_P?usp=sharing](https://colab.research.google.com/drive/1GBC7eLlEM-HqKLUuMcFIQdVuYXzLoS_P?usp=sharing)\n",
    "\n",
    "Importantly, it provides the English to Italian data set I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e35e45b-ffe9-4fe0-a579-357e5a74284d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Jupyter core packages...\n",
      "IPython          : 8.31.0\n",
      "ipykernel        : 6.29.5\n",
      "ipywidgets       : 8.1.5\n",
      "jupyter_client   : 8.6.3\n",
      "jupyter_core     : 5.7.2\n",
      "jupyter_server   : 2.15.0\n",
      "jupyterlab       : 4.3.4\n",
      "nbclient         : 0.10.2\n",
      "nbconvert        : 7.16.5\n",
      "nbformat         : 5.10.4\n",
      "notebook         : 7.3.2\n",
      "qtconsole        : not installed\n",
      "traitlets        : 5.14.3\n"
     ]
    }
   ],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca68a146-e884-4f02-bffd-a8fdba9166a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from letsbuildmodels.devices import get_device\n",
    "from nltk.lm.vocabulary import Vocabulary\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.nn.functional import softmax\n",
    "from torcheval.metrics.functional import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6c6301-f8e8-49d7-a32a-19c62c78e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.00 MB\n",
      "Memory recommended: 21845.34 MB\n"
     ]
    }
   ],
   "source": [
    "def check_memory():\n",
    "    mem_allocated = torch.mps.current_allocated_memory()\n",
    "    mem_recomended = torch.mps.recommended_max_memory()\n",
    "    \n",
    "    print(f\"Memory allocated: {mem_allocated / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Memory recommended: {mem_recomended / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c86ecac-3456-481f-a827-ef9b732913b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/jamescataldo/Code/letsbuildmodels/notebooks/transformer/data/eng_ita_v2.txt. No download needed.\n",
      "File contains 120746 translations\n",
      "English vocabulary size: 4888\n",
      "Italian vocabulary size: 9276\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "\n",
    "# Download the data\n",
    "local_path = os.path.join(os.getcwd(), \"data\", \"eng_ita_v2.txt\")\n",
    "\n",
    "def download_file_if_not_exists():\n",
    "    url = \"https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/datasets/eng_ita_v2.txt\"\n",
    "    \n",
    "    directory = os.path.dirname(local_path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"Downloading file from {url} to {local_path}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        with open(local_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"File already exists at {local_path}. No download needed.\")\n",
    "\n",
    "download_file_if_not_exists()\n",
    "\n",
    "\n",
    "# Read the data\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().strip().split('\\n')\n",
    "    pairs = [tuple([s for s in line.split(' -> ')]) for line in lines]\n",
    "    return pairs\n",
    "\n",
    "pairs = read_data(local_path)\n",
    "print(f\"File contains {len(pairs)} translations\")\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Build vocabularies\n",
    "def build_vocab(pairs):\n",
    "    eng_tokens = list(itertools.chain.from_iterable([word_tokenize(eng) for (eng, _) in pairs]))\n",
    "    ita_tokens = list(itertools.chain.from_iterable([word_tokenize(ita) for (_, ita) in pairs]))\n",
    "    eng_vocab = Vocabulary(eng_tokens, unk_cutoff=2)\n",
    "    ita_vocab = Vocabulary(ita_tokens, unk_cutoff=2)\n",
    "    eng_vocab.update([PAD_TOKEN, EOS_TOKEN])\n",
    "    ita_vocab.update([PAD_TOKEN, EOS_TOKEN, SOS_TOKEN])\n",
    "    return eng_vocab, ita_vocab\n",
    "    \n",
    "eng_vocab, ita_vocab, = build_vocab(pairs)\n",
    "eng_vocab_size = len(eng_vocab) + 1 # The + 1 is to represent the \"<UNK>\" token\n",
    "ita_vocab_size = len(ita_vocab) + 1\n",
    "\n",
    "print('English vocabulary size:', eng_vocab_size)\n",
    "print('Italian vocabulary size:', ita_vocab_size)\n",
    "\n",
    "# Creating integer <-> word mapping\n",
    "class WordMapping:\n",
    "    def __init__(self, vocab):\n",
    "        self.word_to_int = {}\n",
    "        self.int_to_word = {}\n",
    "        word_counts = [(word, vocab[word]) for word in vocab]\n",
    "        sorted_word_counts = sorted(word_counts, key=lambda t: t[1], reverse=True)\n",
    "        sorted_word_counts = sorted_word_counts + [(PAD_TOKEN, 1), (EOS_TOKEN, 1), (SOS_TOKEN, 1)]\n",
    "        for i, (word, _) in enumerate(sorted_word_counts):\n",
    "            self.word_to_int[word] = i\n",
    "            self.int_to_word[i] = word\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if type(key) == str:\n",
    "            if key in self.word_to_int:\n",
    "                return self.word_to_int[key]\n",
    "            elif key.lower() in self.word_to_int:\n",
    "                return self.word_to_int[key.lower()]\n",
    "            else:\n",
    "                return self.word_to_int[UNK_TOKEN]                \n",
    "        elif type(key) == int:\n",
    "            return self.int_to_word[key]\n",
    "        else:\n",
    "            raise KeyError(f\"Invalid key type: {type(key)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_int)\n",
    "\n",
    "eng_mapping = WordMapping(eng_vocab)\n",
    "ita_mapping = WordMapping(ita_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ddcb6f7-be42-446b-a3eb-4633d9bc5366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.00 MB\n",
      "Memory recommended: 21845.34 MB\n"
     ]
    }
   ],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb4b47f-9dae-4d7c-80c6-3e8f38de2e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training translations: 120746\n",
      "Number of batches: 7546\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets and loaders\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng, ita = self.pairs[idx]\n",
    "        eng_tensor = torch.tensor([eng_mapping[word] for word in word_tokenize(eng)]\n",
    "                                  + [eng_mapping[EOS_TOKEN]], dtype=torch.long)\n",
    "        ita_tensor = torch.tensor([ita_mapping[word] for word in word_tokenize(ita)]\n",
    "                                  + [ita_mapping[EOS_TOKEN]], dtype=torch.long)\n",
    "        return eng_tensor, ita_tensor\n",
    "\n",
    "seq_length = 128\n",
    "\n",
    "# Custom collate function to handle padding\n",
    "def collate_fn(batch):\n",
    "    eng_batch, ita_batch = zip(*batch)\n",
    "    def pad(seq, pad_token):\n",
    "        if seq.size()[0] < seq_length:\n",
    "            return torch.cat([seq, torch.full((seq_length - seq.size()[0],), pad_token)])\n",
    "        else:\n",
    "            return seq[:, :seq_length]\n",
    "    eng_batch_padded = torch.stack([pad(x, eng_mapping[PAD_TOKEN]) for x in eng_batch])\n",
    "    ita_batch_padded = torch.stack([pad(x, ita_mapping[PAD_TOKEN]) for x in eng_batch])\n",
    "    return eng_batch_padded, ita_batch_padded\n",
    "\n",
    "# Create the DataLoader\n",
    "translation_dataset = TranslationDataset()\n",
    "translations = len(translation_dataset)\n",
    "indices = list(range(translations))\n",
    "train_indices = indices[::2]\n",
    "test_indices = indices[1::2]\n",
    "train_dataset = Subset(translation_dataset, train_indices)\n",
    "test_dataset = Subset(translation_dataset, test_indices)\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "batches = len(train_dataloader)\n",
    "print(f\"Training translations: {translations}\")\n",
    "print(f\"Number of batches: {batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a11af4-8aff-4129-b48b-38e8420420d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator(\n",
      "  (eng_embedder): Embedding(4888, 128)\n",
      "  (ita_embedder): Embedding(9276, 128)\n",
      "  (pos_enc): PositionalEncoding()\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (disembedder): Linear(in_features=128, out_features=9276, bias=True)\n",
      ")\n",
      "Memory allocated: 25.81 MB\n",
      "Memory recommended: 21845.34 MB\n"
     ]
    }
   ],
   "source": [
    "# Build the models\n",
    "embed_size = 128\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.p = torch.zeros((seq_length, embed_size))\n",
    "        numerator = torch.arange(seq_length, dtype=torch.float).reshape((-1, 1))\n",
    "        exponent = torch.arange(embed_size // 2, dtype=torch.float32) * (2 / embed_size)\n",
    "        denominator = torch.pow(10000, exponent)\n",
    "        quotient = numerator / denominator\n",
    "        self.p[:, 0::2] = torch.sin(quotient).reshape(1, seq_length, embed_size // 2)\n",
    "        self.p[:, 1::2] = torch.cos(quotient).reshape(1, seq_length, embed_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.p.to(x.device)\n",
    "        return x\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eng_embedder = nn.Embedding(eng_vocab_size, embed_size)\n",
    "        self.ita_embedder = nn.Embedding(ita_vocab_size, embed_size)\n",
    "        self.pos_enc = PositionalEncoding()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            num_encoder_layers=3,\n",
    "            num_decoder_layers=3,\n",
    "            batch_first=True)\n",
    "        self.disembedder = nn.Linear(embed_size, ita_vocab_size)\n",
    "        self.tgt_subsequent_mask = None\n",
    "\n",
    "    def get_tgt_subsequent_mask(self, device):\n",
    "        if self.tgt_subsequent_mask is None:\n",
    "            self.tgt_subsequent_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(device)\n",
    "        return self.tgt_subsequent_mask\n",
    "\n",
    "    def forward(self, enc_x, dec_x):\n",
    "        enc_emb = self.eng_embedder(enc_x)\n",
    "        enc_pos = self.pos_enc(enc_emb)\n",
    "        dec_emb = self.ita_embedder(dec_x)\n",
    "        dec_pos = self.pos_enc(dec_emb)\n",
    "        src_padding_mask = (enc_x == eng_mapping[PAD_TOKEN])\n",
    "        tgt_padding_mask = (dec_x == ita_mapping[PAD_TOKEN])\n",
    "        tgt_subsequent_mask = self.get_tgt_subsequent_mask(enc_x.device)\n",
    "        print(\"entering transform\")\n",
    "        check_memory()\n",
    "        transformed = self.transformer(\n",
    "            enc_emb,\n",
    "            dec_pos,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=tgt_subsequent_mask\n",
    "        )\n",
    "        print(\"exiting transform\")\n",
    "        output = softmax(self.disembedder(transformed), dim=2)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Translator()\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "print(model)\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d00a059-4af2-41d7-a6aa-959ab970a94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 25.81 MB\n",
      "Memory recommended: 21845.34 MB\n"
     ]
    }
   ],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0da222-44c5-44b5-9280-81734facdd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 25.84 MB\n",
      "Memory recommended: 21845.34 MB\n",
      "Target size: torch.Size([8, 128])\n",
      "entering transform\n",
      "Memory allocated: 27.92 MB\n",
      "Memory recommended: 21845.34 MB\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=ita_mapping[PAD_TOKEN])\n",
    "    optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09)\n",
    "    num_epochs = 3\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        count = 0\n",
    "        for i, (encoder_input, target) in enumerate(train_dataloader):\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = torch.empty_like(target).to(device)\n",
    "            decoder_input[:, 0] = ita_mapping[SOS_TOKEN]\n",
    "            decoder_input[:, 1:] = target[:, :-1]\n",
    "            target = target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            check_memory()\n",
    "            print(f\"Target size: {target.size()}\")\n",
    "            output = model(encoder_input, decoder_input)\n",
    "            check_memory()\n",
    "\n",
    "            print(f\"Output type: {output.dtype}, Target size: {target.dtype}\")\n",
    "            loss = loss_fn(target, output)\n",
    "            print(\"Ran once\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() / target.size()[1]\n",
    "            count += 1\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {i}/{batches}, Loss: {epoch_loss / count:.4f}\", end=\"\\r\")\n",
    "            \n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss / count:.4f}                             \")\n",
    "                \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a960ba0-b4ce-4a6a-8b0c-a377b28d4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = {\n",
    "    PAD_TOKEN,\n",
    "    EOS_TOKEN,\n",
    "    SOS_TOKEN,\n",
    "    UNK_TOKEN,\n",
    "}\n",
    "\n",
    "def to_ita_sentence(tensor):\n",
    "    ita = [ita_mapping[x.item()] for x in tensor]\n",
    "    strs = [y for y in ita if y not in specials]\n",
    "    return \" \".join(strs)\n",
    "\n",
    "def to_eng_sentence(tensor):\n",
    "    eng = [eng_mapping[x.item()] for x in tensor]\n",
    "    strs = [y for y in eng if y not in specials]\n",
    "    return \" \".join(strs).replace(\" '\", \"'\")\n",
    "\n",
    "def test(print_translations=False):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (encoder_input, target) in enumerate(test_dataloader):\n",
    "            encoder_input, target = encoder_input.to(device), target.to(device)\n",
    "            \n",
    "            output = model.translate(encoder_input, target.size()[1])\n",
    "    \n",
    "            for batch in range(batch_size):\n",
    "                input_tokens = encoder_input[batch]\n",
    "                input_str = to_eng_sentence(input_tokens)\n",
    "                output_str = to_ita_sentence(output[batch])            \n",
    "                target_tokens = target[batch]\n",
    "                target_str = to_ita_sentence(target_tokens) \n",
    "                if print_translations:\n",
    "                    print(f\"English: {input_str}\")\n",
    "                    print(f\"Desired Italian: {target_str}\")\n",
    "                    print(f\"Generated Italian: {output_str}\")\n",
    "                    print()\n",
    "    \n",
    "                loss = bleu_score(output_str, [target_str], n_gram=min(2, len(output_str)))\n",
    "        \n",
    "                epoch_loss += loss.item()\n",
    "                count += 1\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Batch {i}/{batches}, BLEU: {epoch_loss / count:.4f}\", end=\"\\r\")\n",
    "        print(f\"BLEU: {epoch_loss / count:.4f}                             \")\n",
    "                \n",
    "# test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
